{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bdb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers #Installed package from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in /home/peter/.local/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /home/peter/.local/lib/python3.10/site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: packaging in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (59.6.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: namex in /home/peter/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: rich in /home/peter/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: optree in /home/peter/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/peter/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/peter/.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/peter/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/peter/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/peter/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/peter/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4917c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-24 10:49:02.387835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748076542.402595    3791 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748076542.407009    3791 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748076542.419661    3791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076542.419697    3791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076542.419701    3791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076542.419702    3791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 10:49:02.424076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random #I want the model to pick a random quote.\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from sentence_transformers import SentenceTransformer #Importing from Hugging face so i can use a local model for embedding.\n",
    "from pypdf import PdfReader\n",
    "import nltk #Importing so i can get whole quotes (rows) even if there is . in the sentence.\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"punkt_tab\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity #Importing for semantic searching.\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"API_KEY\"))\n",
    "model=SentenceTransformer('all-MiniLM-L6-v2') #Model used for local embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbcb47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model, trained by Google.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content( #Running to see if the model is working, and can leaave me a response to my question.\n",
    "model=\"gemini-2.0-flash\",\n",
    "contents=\"Hi, who are you?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8844764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 376 0 (offset 0)\n",
      "Ignoring wrong pointing object 393 0 (offset 0)\n",
      "Ignoring wrong pointing object 425 0 (offset 0)\n",
      "Ignoring wrong pointing object 427 0 (offset 0)\n",
      "Ignoring wrong pointing object 434 0 (offset 0)\n",
      "Ignoring wrong pointing object 652 0 (offset 0)\n",
      "Ignoring wrong pointing object 678 0 (offset 0)\n",
      "Ignoring wrong pointing object 781 0 (offset 0)\n",
      "Ignoring wrong pointing object 837 0 (offset 0)\n",
      "Ignoring wrong pointing object 840 0 (offset 0)\n",
      "Ignoring wrong pointing object 843 0 (offset 0)\n",
      "Ignoring wrong pointing object 854 0 (offset 0)\n",
      "Ignoring wrong pointing object 885 0 (offset 0)\n",
      "Ignoring wrong pointing object 929 0 (offset 0)\n",
      "Ignoring wrong pointing object 1050 0 (offset 0)\n",
      "Ignoring wrong pointing object 1092 0 (offset 0)\n",
      "Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "Ignoring wrong pointing object 1138 0 (offset 0)\n",
      "Ignoring wrong pointing object 1140 0 (offset 0)\n",
      "Ignoring wrong pointing object 1149 0 (offset 0)\n",
      "Ignoring wrong pointing object 1157 0 (offset 0)\n",
      "Ignoring wrong pointing object 1161 0 (offset 0)\n",
      "Ignoring wrong pointing object 1163 0 (offset 0)\n",
      "Ignoring wrong pointing object 1173 0 (offset 0)\n",
      "Ignoring wrong pointing object 1200 0 (offset 0)\n",
      "Ignoring wrong pointing object 1202 0 (offset 0)\n",
      "Ignoring wrong pointing object 1276 0 (offset 0)\n",
      "Ignoring wrong pointing object 1280 0 (offset 0)\n",
      "Ignoring wrong pointing object 1290 0 (offset 0)\n",
      "Ignoring wrong pointing object 1425 0 (offset 0)\n",
      "Ignoring wrong pointing object 1452 0 (offset 0)\n",
      "Ignoring wrong pointing object 1512 0 (offset 0)\n",
      "Ignoring wrong pointing object 1551 0 (offset 0)\n",
      "Ignoring wrong pointing object 1578 0 (offset 0)\n",
      "Ignoring wrong pointing object 1588 0 (offset 0)\n",
      "Ignoring wrong pointing object 1645 0 (offset 0)\n",
      "Ignoring wrong pointing object 1650 0 (offset 0)\n",
      "Ignoring wrong pointing object 1674 0 (offset 0)\n",
      "Ignoring wrong pointing object 1677 0 (offset 0)\n",
      "Ignoring wrong pointing object 1725 0 (offset 0)\n",
      "Ignoring wrong pointing object 1735 0 (offset 0)\n",
      "Ignoring wrong pointing object 1753 0 (offset 0)\n",
      "Ignoring wrong pointing object 1755 0 (offset 0)\n",
      "Ignoring wrong pointing object 1793 0 (offset 0)\n",
      "Ignoring wrong pointing object 1795 0 (offset 0)\n",
      "Ignoring wrong pointing object 1952 0 (offset 0)\n",
      "Ignoring wrong pointing object 1969 0 (offset 0)\n",
      "Ignoring wrong pointing object 1978 0 (offset 0)\n",
      "Ignoring wrong pointing object 1990 0 (offset 0)\n",
      "Ignoring wrong pointing object 2059 0 (offset 0)\n",
      "Ignoring wrong pointing object 2068 0 (offset 0)\n",
      "Ignoring wrong pointing object 2076 0 (offset 0)\n",
      "Ignoring wrong pointing object 2079 0 (offset 0)\n",
      "Ignoring wrong pointing object 2081 0 (offset 0)\n",
      "Ignoring wrong pointing object 2083 0 (offset 0)\n",
      "Ignoring wrong pointing object 2085 0 (offset 0)\n",
      "Ignoring wrong pointing object 2087 0 (offset 0)\n",
      "Ignoring wrong pointing object 2089 0 (offset 0)\n",
      "Ignoring wrong pointing object 2091 0 (offset 0)\n",
      "Ignoring wrong pointing object 2093 0 (offset 0)\n",
      "Ignoring wrong pointing object 2095 0 (offset 0)\n",
      "Ignoring wrong pointing object 2097 0 (offset 0)\n",
      "Ignoring wrong pointing object 2099 0 (offset 0)\n",
      "Ignoring wrong pointing object 2101 0 (offset 0)\n",
      "Ignoring wrong pointing object 2103 0 (offset 0)\n",
      "Ignoring wrong pointing object 2105 0 (offset 0)\n",
      "Ignoring wrong pointing object 2107 0 (offset 0)\n",
      "Ignoring wrong pointing object 2280 0 (offset 0)\n",
      "Ignoring wrong pointing object 2298 0 (offset 0)\n",
      "Ignoring wrong pointing object 2300 0 (offset 0)\n",
      "Ignoring wrong pointing object 2302 0 (offset 0)\n",
      "Ignoring wrong pointing object 2306 0 (offset 0)\n",
      "Ignoring wrong pointing object 2310 0 (offset 0)\n",
      "Ignoring wrong pointing object 2329 0 (offset 0)\n",
      "Ignoring wrong pointing object 2331 0 (offset 0)\n",
      "Ignoring wrong pointing object 2333 0 (offset 0)\n",
      "Ignoring wrong pointing object 2337 0 (offset 0)\n",
      "Ignoring wrong pointing object 2369 0 (offset 0)\n",
      "Ignoring wrong pointing object 2371 0 (offset 0)\n",
      "Ignoring wrong pointing object 2373 0 (offset 0)\n",
      "Ignoring wrong pointing object 2377 0 (offset 0)\n",
      "Ignoring wrong pointing object 2427 0 (offset 0)\n",
      "Ignoring wrong pointing object 2436 0 (offset 0)\n",
      "Ignoring wrong pointing object 2443 0 (offset 0)\n",
      "Ignoring wrong pointing object 2456 0 (offset 0)\n",
      "Ignoring wrong pointing object 2459 0 (offset 0)\n",
      "Ignoring wrong pointing object 2472 0 (offset 0)\n",
      "Ignoring wrong pointing object 2474 0 (offset 0)\n",
      "Ignoring wrong pointing object 2488 0 (offset 0)\n",
      "Ignoring wrong pointing object 2490 0 (offset 0)\n",
      "Ignoring wrong pointing object 2493 0 (offset 0)\n",
      "Ignoring wrong pointing object 2545 0 (offset 0)\n",
      "Ignoring wrong pointing object 2559 0 (offset 0)\n",
      "Ignoring wrong pointing object 2569 0 (offset 0)\n",
      "Ignoring wrong pointing object 2594 0 (offset 0)\n",
      "Ignoring wrong pointing object 2629 0 (offset 0)\n",
      "Ignoring wrong pointing object 2773 0 (offset 0)\n",
      "Ignoring wrong pointing object 2886 0 (offset 0)\n",
      "Ignoring wrong pointing object 2921 0 (offset 0)\n",
      "Ignoring wrong pointing object 2949 0 (offset 0)\n",
      "Ignoring wrong pointing object 2968 0 (offset 0)\n",
      "Ignoring wrong pointing object 2974 0 (offset 0)\n",
      "Ignoring wrong pointing object 2996 0 (offset 0)\n",
      "Ignoring wrong pointing object 3012 0 (offset 0)\n",
      "Ignoring wrong pointing object 3128 0 (offset 0)\n",
      "Ignoring wrong pointing object 3159 0 (offset 0)\n",
      "Ignoring wrong pointing object 3171 0 (offset 0)\n",
      "Ignoring wrong pointing object 3189 0 (offset 0)\n",
      "Ignoring wrong pointing object 3254 0 (offset 0)\n",
      "Ignoring wrong pointing object 3257 0 (offset 0)\n",
      "Ignoring wrong pointing object 3260 0 (offset 0)\n",
      "Ignoring wrong pointing object 3263 0 (offset 0)\n",
      "Ignoring wrong pointing object 3329 0 (offset 0)\n",
      "Ignoring wrong pointing object 3331 0 (offset 0)\n",
      "Ignoring wrong pointing object 3336 0 (offset 0)\n",
      "Ignoring wrong pointing object 3338 0 (offset 0)\n",
      "Ignoring wrong pointing object 3399 0 (offset 0)\n",
      "Ignoring wrong pointing object 3411 0 (offset 0)\n",
      "Ignoring wrong pointing object 3434 0 (offset 0)\n",
      "Ignoring wrong pointing object 3464 0 (offset 0)\n",
      "Ignoring wrong pointing object 3474 0 (offset 0)\n",
      "Ignoring wrong pointing object 3485 0 (offset 0)\n",
      "Ignoring wrong pointing object 3516 0 (offset 0)\n",
      "Ignoring wrong pointing object 3528 0 (offset 0)\n",
      "Ignoring wrong pointing object 3577 0 (offset 0)\n",
      "Ignoring wrong pointing object 3593 0 (offset 0)\n",
      "Ignoring wrong pointing object 3603 0 (offset 0)\n",
      "Ignoring wrong pointing object 3624 0 (offset 0)\n",
      "Ignoring wrong pointing object 3639 0 (offset 0)\n",
      "Ignoring wrong pointing object 3644 0 (offset 0)\n",
      "Ignoring wrong pointing object 3735 0 (offset 0)\n",
      "Ignoring wrong pointing object 3885 0 (offset 0)\n",
      "Ignoring wrong pointing object 3889 0 (offset 0)\n",
      "Ignoring wrong pointing object 3911 0 (offset 0)\n",
      "Ignoring wrong pointing object 3934 0 (offset 0)\n",
      "Ignoring wrong pointing object 3969 0 (offset 0)\n",
      "Ignoring wrong pointing object 3979 0 (offset 0)\n",
      "Ignoring wrong pointing object 4033 0 (offset 0)\n",
      "Ignoring wrong pointing object 4036 0 (offset 0)\n",
      "Ignoring wrong pointing object 4038 0 (offset 0)\n",
      "Ignoring wrong pointing object 4102 0 (offset 0)\n",
      "Ignoring wrong pointing object 4311 0 (offset 0)\n",
      "Ignoring wrong pointing object 4313 0 (offset 0)\n",
      "Ignoring wrong pointing object 4315 0 (offset 0)\n",
      "Ignoring wrong pointing object 4356 0 (offset 0)\n",
      "Ignoring wrong pointing object 4434 0 (offset 0)\n",
      "Ignoring wrong pointing object 4436 0 (offset 0)\n",
      "Ignoring wrong pointing object 4439 0 (offset 0)\n",
      "Ignoring wrong pointing object 4443 0 (offset 0)\n",
      "Ignoring wrong pointing object 4446 0 (offset 0)\n",
      "Ignoring wrong pointing object 4449 0 (offset 0)\n",
      "Ignoring wrong pointing object 4452 0 (offset 0)\n",
      "Ignoring wrong pointing object 4455 0 (offset 0)\n",
      "Ignoring wrong pointing object 4612 0 (offset 0)\n",
      "Ignoring wrong pointing object 4631 0 (offset 0)\n",
      "Ignoring wrong pointing object 4674 0 (offset 0)\n",
      "Ignoring wrong pointing object 4720 0 (offset 0)\n",
      "Ignoring wrong pointing object 4723 0 (offset 0)\n",
      "Ignoring wrong pointing object 4731 0 (offset 0)\n",
      "Ignoring wrong pointing object 4734 0 (offset 0)\n",
      "Ignoring wrong pointing object 4803 0 (offset 0)\n",
      "Ignoring wrong pointing object 4826 0 (offset 0)\n",
      "Ignoring wrong pointing object 4852 0 (offset 0)\n",
      "Ignoring wrong pointing object 4946 0 (offset 0)\n",
      "Ignoring wrong pointing object 4969 0 (offset 0)\n",
      "Ignoring wrong pointing object 5060 0 (offset 0)\n",
      "Ignoring wrong pointing object 5106 0 (offset 0)\n",
      "Ignoring wrong pointing object 5108 0 (offset 0)\n",
      "Ignoring wrong pointing object 5144 0 (offset 0)\n",
      "Ignoring wrong pointing object 5168 0 (offset 0)\n",
      "Ignoring wrong pointing object 5177 0 (offset 0)\n",
      "Ignoring wrong pointing object 5180 0 (offset 0)\n",
      "Ignoring wrong pointing object 5213 0 (offset 0)\n",
      "Ignoring wrong pointing object 5237 0 (offset 0)\n",
      "Ignoring wrong pointing object 5252 0 (offset 0)\n",
      "Ignoring wrong pointing object 5270 0 (offset 0)\n",
      "Ignoring wrong pointing object 5334 0 (offset 0)\n",
      "Ignoring wrong pointing object 5338 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "reader_stat = PdfReader(\"pdf/islr.pdf\") #islr.pdf is the boook, An Introduction to Statistical Learning by Gareth James and more.\n",
    "text_stat = \"\"\n",
    "for page in reader_stat.pages:\n",
    "    text_stat += page.extract_text()\n",
    "\n",
    "reader_homl = PdfReader(\"pdf/homl.pdf\") #homl.pdf is the boook, Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow (ML/DL).\n",
    "text_homl = \"\"\n",
    "for page in reader_homl.pages:\n",
    "    text_homl += page.extract_text()\n",
    "\n",
    "reader_lotr = PdfReader(\"pdf/lotr.pdf\") #lotr.pdf is a pdf with some famous Lord of the ring quotes.\n",
    "text_lotr = \"\"\n",
    "for page in reader_lotr.pages:\n",
    "    text_lotr += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c0fa814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunking(text, max_tokens=100): #Creating funtion for Semantic chunking to use in next step.\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Check if adding the sentence keeps the chunk within the token limit\n",
    "        if len((current_chunk + \" \" + sentence).split()) <= max_tokens:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a973f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_stat = semantic_chunking(text_stat, max_tokens=100)\n",
    "chunks_homl = semantic_chunking(text_homl, max_tokens=100)\n",
    "\n",
    "sentences_lotr = sent_tokenize(text_lotr) #Not using Semantic search because i only have short quotes in this data.\n",
    "sentences_lotr = [s.strip() for s in sentences_lotr if len(s.strip()) > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff98cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAT chunks: 2530\n",
      "HOML chunks: 3130\n",
      "TOTAL chunks: 5660\n"
     ]
    }
   ],
   "source": [
    "print(\"STAT chunks:\", len(chunks_stat))\n",
    "print(\"HOML chunks:\", len(chunks_homl))\n",
    "print(\"TOTAL chunks:\", len(chunks_stat) + len(chunks_homl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2dd9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skipping broken chunk in index 491 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 902 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 965 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 968 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 995 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 1007 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 1296 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 2148 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 2150 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 2179 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n"
     ]
    }
   ],
   "source": [
    "valid_chunks_homl = [] #Had problem with creating embedding for homl, need to clean out bad/non string chunks.\n",
    "\n",
    "for i, chunk in enumerate(chunks_homl):\n",
    "    try:\n",
    "        model.encode([chunk])  # testing one each turn\n",
    "        valid_chunks_homl.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\" Skipping broken chunk in index {i} → {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f5707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 98/98 [00:03<00:00, 30.86it/s]\n",
      "Batches: 100%|██████████| 80/80 [00:02<00:00, 28.24it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 101.73it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_homl = model.encode(valid_chunks_homl, show_progress_bar=True) #Using the filtered/working chunks for homl.\n",
    "embeddings_stat = model.encode(chunks_stat, show_progress_bar=True)\n",
    "embeddings_lotr = model.encode(sentences_lotr, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfb3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating save function for easier saving.\n",
    "def save(texts, embeddings, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump([{\"text\": t, \"embedding\": e.tolist()} for t, e in zip(texts, embeddings)], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Savinig embeddings for later use.\n",
    "save(chunks_stat, embeddings_stat, \"embeddings/embeddings_stat.json\")\n",
    "save(valid_chunks_homl, embeddings_homl, \"embeddings/embeddings_homl.json\")\n",
    "save(sentences_lotr, embeddings_lotr, \"embeddings/embeddings_lotr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f704399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrote this code so i can quickly load embeddings without running all code above again. Saves time if i close the project and need to come back.\n",
    "with open(\"embeddings/embeddings_stat.json\") as f:\n",
    "    data_stat = json.load(f)\n",
    "texts_stat = [d[\"text\"] for d in data_stat]\n",
    "embeddings_stat = np.array([d[\"embedding\"] for d in data_stat])\n",
    "\n",
    "with open(\"embeddings/embeddings_homl.json\") as f:\n",
    "    data_homl = json.load(f)\n",
    "texts_homl = [d[\"text\"] for d in data_homl]\n",
    "embeddings_homl = np.array([d[\"embedding\"] for d in data_homl])\n",
    "\n",
    "with open(\"embeddings/embeddings_lotr.json\") as f:\n",
    "    data_lotr = json.load(f)\n",
    "texts_lotr = [d[\"text\"] for d in data_lotr]\n",
    "embeddings_lotr = np.array([d[\"embedding\"] for d in data_lotr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagging text so i know where the output came frome if i want to use it later.\n",
    "texts_stat_tagged = [f\"[STAT] {text}\" for text in texts_stat]\n",
    "texts_homl_tagged = [f\"[HOML] {text}\" for text in texts_homl]\n",
    "\n",
    "#Combining both stat and homl so the model can search in both.\n",
    "texts_combined = texts_stat_tagged + texts_homl_tagged\n",
    "embeddings_combined = np.vstack([embeddings_stat, embeddings_homl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57855a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, texts, embeddings, model, k=5): #Perform semantic search to find the chunks most similar in meaning to the user's question\n",
    "\n",
    "    query_emb = model.encode([query])\n",
    "    similarity_scores = cosine_similarity(query_emb, embeddings)[0]\n",
    "    top_indices = similarity_scores.argsort()[-k:][::-1]\n",
    "    return [(texts[i], similarity_scores[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This prompt defines how the model should behave and respond to questions\n",
    "system_prompt = \"\"\"You are a helpful assistant who explains statistical and machine learning concepts clearly.\n",
    "\n",
    "You will receive a question along with some context. You must base your answer **only on the provided context**, and not on any external knowledge.\n",
    "\n",
    "If the context does not contain enough information to answer the question, simply reply:  \n",
    "**\"I don't know.\"** Do not attempt to guess or invent details.\n",
    "\n",
    "Please write your answer in clear, simple language, and structure it into well-formed paragraphs.\n",
    "\n",
    "Use a slightly whimsical tone, as if you were Gandalf explaining it over a campfire.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "To make it more entertaining, include a Lord of the Rings quote that fits the context. You may **tweak the quote slightly** to make it funnier or more related to the topic.\n",
    "\n",
    "At the end of your answer, include the quote on its own line, followed by the name of the character who said it.\n",
    "For example:\n",
    "\"There is always hope.\" – Aragorn\n",
    "\n",
    "Quote:\n",
    "{lotr_quote}\" – {lotr_author}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e7ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(query, texts, embeddings, model, k=5): # Generates a user prompt by retrieving the most relevant context chunks based on semantic similarity\n",
    "    context_chunks = semantic_search(query, texts, embeddings, model, k)\n",
    "    context = \"\\n\\n\".join([chunk for chunk, score in context_chunks])\n",
    "    return f\"Question: {query}\\n\\nContext:\\n{context}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d655a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lotr_author(quote): #I want to present the author of the quote later on so thats why I extract it seperatly.\n",
    "    if \"–\" in quote:\n",
    "        return quote.split(\"–\")[-1].strip()\n",
    "    elif \"-\" in quote:\n",
    "        return quote.split(\"-\")[-1].strip()\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5135b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates a response from the Gemini model using the user question, selected context, and a LOTR quote\n",
    "def generate_response(system_prompt, query, texts, embeddings, model, client, lotr_quotes, gemini_model=\"gemini-2.0-flash\"): \n",
    "    lotr_quote = random.choice(lotr_quotes)\n",
    "    user_prompt = generate_user_prompt(query, texts, embeddings, model)\n",
    "\n",
    "    filled_prompt = system_prompt.format(\n",
    "        context=user_prompt.split(\"Context:\\n\")[1],\n",
    "        query=query,\n",
    "        lotr_quote=lotr_quote,\n",
    "        lotr_author = extract_lotr_author(lotr_quote)\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "    model=gemini_model,\n",
    "    contents=filled_prompt\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bdf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halt, traveler! Gather 'round the fire, and let's speak of the Normal Distribution, a shape as familiar to statisticians as the Shire is to Hobbits.\n",
      "\n",
      "Imagine a bell, a grand, symmetrical bell. That's the shape of a Normal Distribution, also known as a Gaussian distribution. It describes how the values of a feature, like the heights of Elves or the prices of Lembas bread, are spread out. Most values cluster around the average, the mean – that's the peak of our bell. Then, as you move away from the mean, the values become less common, tapering off on either side.\n",
      "\n",
      "There's even a rule of thumb to remember, a bit like an Elven rhyme: \"68-95-99.7\". About 68% of the values fall within one standard deviation (σ) of the mean, 95% within two σ, and a whopping 99.7% within three σ. This \"standard deviation\" measures how spread out the values are. A small standard deviation means the bell is narrow and tall, most values are close to the mean. A large standard deviation means the bell is wide and flat, the values are more scattered.\n",
      "\n",
      "Indeed, the Normal Distribution is everywhere, from the random vectors sampled by K.random_normal() with a mean μ and standard deviation σ, to the distribution of test statistics under the null hypothesis in statistical tests. Even the humble t-distribution, when its \"n\" value surpasses around 30, starts resembling the shape of our beloved bell.\n",
      "\n",
      "\"This distribution was appointed to you, and if you do not find a way, no one will.\" – Gandalf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_response( #Wanted to see how the model answers.\n",
    "    system_prompt,\n",
    "    query=\"What is Normal Distribution?\",\n",
    "    texts=texts_combined,\n",
    "    embeddings=embeddings_combined,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    lotr_quotes=texts_lotr\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a24e9",
   "metadata": {},
   "source": [
    "<h3>Evaluation of the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be2ee3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = [ #Creating questions with ideal answers so i can evalute the models performance.\n",
    "{\"question\": \"What is a common performance metric for regression problems?\",\n",
    " \"ideal_answer\": \"A typical performance measure for regression problems is the Root Mean Square Error (RMSE).\"\n",
    "},\n",
    "{\"question\": \"When we fit a linear regression model to a particular data set, many problems may occur. which is the most common?:\",\n",
    " \"ideal_answer\": \"Common problems in linear regression include non-linearity, heteroscedasticity, correlated errors, outliers, high-leverage points, and collinearity.\"\n",
    "},\n",
    "{\"question\": \"How many horses are in Lord of the Rings?\",\n",
    " \"ideal_answer\": \"There is no relevant information about horses in the provided context.\"\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#created an evalution prompt so i can measure the models performance. This answers if i need to tweak it or not.\n",
    "evaluation_system_prompt = \"\"\"You are an intelligent evaluation system that grades the quality of an AI assistant's answer.\n",
    "\n",
    "You will be given:\n",
    "- A user question\n",
    "- An ideal answer (the key facts that must be included)\n",
    "- The assistant's generated answer (which may have a whimsical tone)\n",
    "\n",
    "Your task is to evaluate the assistant's answer based on factual correctness and coverage of key ideas, **regardless of style**.\n",
    "\n",
    "Score:\n",
    "- 1.0 if the assistant includes the key information from the ideal answer, even if expressed creatively or with humor\n",
    "- 0.5 if the answer is partially correct or misses important parts\n",
    "- 0.0 if the answer is incorrect or irrelevant\n",
    "\n",
    "Also include a brief motivation for your score.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7ad8f",
   "metadata": {},
   "source": [
    "Testing the model on a questions that does exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "740937f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n",
      "\n",
      "Motivation:The AI assistant correctly identifies Root Mean Square Error (RMSE) as a common performance metric for regression problems. It also mentions Mean Squared Error (MSE) which is closely related and also correct. The answer is creative, but contains the correct information.\n"
     ]
    }
   ],
   "source": [
    "query = validation_data[0][\"question\"]\n",
    "response = generate_response(\n",
    "    system_prompt=system_prompt,\n",
    "    query=query,\n",
    "    texts=texts_combined,\n",
    "    embeddings=embeddings_combined,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    lotr_quotes=texts_lotr\n",
    ")\n",
    "\n",
    "evaluation_prompt = f\"\"\"Question: {query}\n",
    "AI-assistent answer: {response}\n",
    "Ideal Answer: {validation_data[0]['ideal_answer']}\"\"\"\n",
    "\n",
    "evaluation_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=evaluation_prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=evaluation_system_prompt,\n",
    "        temperature=0.0\n",
    "    )\n",
    ")\n",
    "\n",
    "print(evaluation_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34eb79",
   "metadata": {},
   "source": [
    "Testing the model on a questions that does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab47dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many horses are in Lord of the Rings?',\n",
       " 'ideal_answer': 'There is no relevant information about horses in the provided context.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14f8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n",
      "\n",
      "Motivation:The AI assistant correctly states that it does not know the answer to the question. It also acknowledges the lack of relevant information. The whimsical tone does not detract from the accuracy of the response.\n"
     ]
    }
   ],
   "source": [
    "query = validation_data[2][\"question\"]\n",
    "response = generate_response(\n",
    "    system_prompt=system_prompt,\n",
    "    query=query,\n",
    "    texts=texts_combined,\n",
    "    embeddings=embeddings_combined,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    lotr_quotes=texts_lotr\n",
    ")\n",
    "\n",
    "evaluation_prompt = f\"\"\"Question: {query}\n",
    "AI-assistent answer: {response}\n",
    "Ideal Answer: {validation_data[2]['ideal_answer']}\"\"\"\n",
    "\n",
    "evaluation_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=evaluation_prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=evaluation_system_prompt,\n",
    "        temperature=0.0\n",
    "    )\n",
    ")\n",
    "\n",
    "print(evaluation_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fef6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving model so i can use it later.\n",
    "model.save(\"model/my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24c6ea",
   "metadata": {},
   "source": [
    "<h3>Fördjupad och kritisk diskussion</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e1f97",
   "metadata": {},
   "source": [
    "Användningsområden:<br>\n",
    "På ett företag där personer som arbetar inom området AI, ML och med statistik. Modellen kan hjälpa medarbetarna i deras dagliga verksamhet genom att fungera som ett stöd där man kan ställa frågor och få svar för att lösa olika uppgifter.\n",
    "\n",
    "På en skola eller annan verksamhet som arbetar med utbildning där både lärare och elever skulle kunna nyttja modellen för sitt lärande. Allt från att lära nytt, reptera eller ställa frågor till modellen om sådant man vill lösa.\n",
    "\n",
    "Möjligheter:<br>\n",
    "Spara tid vilket också är relaterat till pengar. De anställda kanske genom den frigjorda tiden kan ägna sig åt andra saker eller överlag kan lägga mer tid på det dem önskar och har behov av. Botten kan ge fördjupad förståelse och personlig utveckling för användaren. Möjlighet till repetion. Eventullt erbjuda detta som tjänst som verksamheten kan ta betalt för genom att andra utanför verksamheten kan nyttja den, så som stundenter eller kunder.\n",
    "\n",
    "Risker:<br>\n",
    "Självklart finns olika risker genom att bygga och implementera en sådan modell och även användadet av den.\n",
    "Hur hanterar man känslig information? Skall det vara en modell som endast förekommer lokalt till följd av känslig information eller skall man kunna nyttja den i molnet? Vem skall ha tillgång till modellen? Skall även andra inom verksamheten kunna nyttja modellen eller endast ett urval? Hur kan vi \"låsa\" modellen eller på annat sätt begränsa den så att den inte sprids? Vilken typ av frågor eller känslig information får användarna lov att mata in i modellen? Vem skall har rättigheter att kunna fortsätta träna modellen?\n",
    "Hur håller vi modellen uppdaterad med rätt och korrekt information? Det finns många frågeställningar att fundera över när det kommer till risker med modellen, men då möjligheterna är många och fördelarna troligen gynnsamma är det definitivt värt att arbeta fram en plan för att försöka minimera riskerna så att man kan implementera modellen.\n",
    "\n",
    "Lagring:<br>\n",
    "Hur skall vi lagra modellen? Skall det vara i molnet eller skall det vara lokalt inom verksamheten? Som vi vet finns både fördelar och nackadelar med detta. Så som tillgänlighet och kostnader. Beroende på verksamhet och de ekonomiska resursena samt känsligheten (syftar på om datan är känslig för verksamheten) för modellen, styr vilken lösning som bäst lämpar sig för varje organisation och hur den lagras och implementeras. \n",
    "\n",
    "Kostnader:<br>\n",
    "Kostnaderna ser olika ut för om man använder modellen lokalt eller molnbaserat och här behöver man som verksamhet också ta ställning till tillgänglighet, utveckling, säkerhet resurser med mera som får styra hur man väljer att arbeta med och implementera sin modell. Skall vman använda en molnbaserad modell för att ständigt ha tillgång till de senaste utvecklingarna? Fördelen är att man har har tillgång till ny teknik men en nackdel är att det kan fort kosta väldigt mycket pengar om man behöver skicka mycket förfrågningar över API. Använder man lokala lösningar så kan man komma undan mycket kostnader för API men behöver stället själv lägga mer kostnader på hårdvara och personal som till och från behöver se till att modellen uppdateras efter behov när ny teknik är tillgänglig. Även här blir det tydligt att de ekonomiska musklerna och tillgängligheten blir styrande. \n",
    "\n",
    "Förhållningssätt :<br>\n",
    "De allra flesta verksamheter kräver att de anställda skall efterleva deras kultur och språk, grafiska image med mera. Det är också något vi behöver tänka på vad gäller modellen och hur den skall vara utformad och hur den svarar. Det kan därför vara viktigt att utforma modellen så att den svarar som vi önskar, använder ett korrekt språk, följer normer och attityder som gäller i verksamheten.\n",
    "\n",
    "Copyright/rättigheter:<br>\n",
    "För att kunna nyttja modellen i verksamheten är det också väldigt viktigt att man har rättigheter till att använda det material som modellen tränas på så att detta sedan kan användas legitimt i verksamheten. Detta för att undvika juridiska påföljder så som stämningar eller andra åberopande som kan skada verksamheten både ekonomiskt men också förtroendet.\n",
    "\n",
    "Förbättringar:<br>\n",
    "En vidare utveckling av modellen skulle vara att den också kan bearbeta och hantera bilder i datan. Detta för att kunna modellen skall kunna inkludera bilder i sina svar vilket bidrar till en ökad förståelse när detta kombineras med texten. Två andra idéer skulle dels kunna vara att den kan tränas att svara på flera språk vilket gör det enklare för användaren att ta till sig informationen. Dels att koppla till en berättande röst som läser upp svaret för användaren som den så önskar. Skulle man spinna vidare på den lite roligare modellen så skulle man med fördel kunna nyttja en röst som låter som Gandalf vilket hade bidragit med lite extra humor till svaret.\n",
    "\n",
    "Reflektioner:<br>\n",
    "I praktiken kan det gå relativt snabbt att skapa och sjösätta en ChatBot som vi kan nyttja inom verksamheten. Men som nämnt ovan finns det en hel del att förhålla sig till. Det kan därför vara bra att man innan man bygger en ChatBot har funderat åtminstone lite kring vissa av de nämnda områdena och förankrat detta med berörda och beslutsfattare. För även om det är roligt att sätta igång och bygga en modell så kan det bli så att den inte kan nyttjas om den inte uppfyller de krav som ställs. Skulle man redan byggt en modell som inte kan användas så har man troligen redan lagt ner både mycket tid och resurser vilka går förlorade och som hade kunna nyttjats til något annat. Samtidigt om det är så att man funderat kring och bygger en ChatBot som faktiskt används inom verksamheten så finns där fantastiska möjligheter för verksamheten genom nyttjandet av en sådan. Med tanke på den utveckling vi ser och hur Chatbotar blir allt vanligare är det troligt att de olika organisationerna och företagen ser att nyttan överväger riskerna vilket gör det till ett spännande område att fördjupa sig i.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f7b69",
   "metadata": {},
   "source": [
    "<h3>Teoretiska frågor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317d8a5",
   "metadata": {},
   "source": [
    "1. AI, maskininlärning (ML) och djupinlärning (DL) är relaterade där AI är det breda området, ML är en underkategori av AI, och DL är en vidareutveckling av ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c5578",
   "metadata": {},
   "source": [
    "2. TensorFlow är ramverket som tränar modellen, och Keras är ett förenklat lager ovanpå som låter en bygga och träna modeller genom ett lättanvänt API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d10d1",
   "metadata": {},
   "source": [
    "3. Hyperparametrar styr hur modellen tränas. Parametrar är vad modellen lär sig under träningen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc1b80",
   "metadata": {},
   "source": [
    "4. Träningsdata används för att träna modellen, så att den kan optimera sina beräkningar för att få till bästa generalisering.\n",
    "\n",
    "    Valideringsdata används för att utvärdera modellens/modellernas prestation vilket ofta används för att välja vilken modell vi fortsatt vill använda.\n",
    "\n",
    "    Testdata används för att se hur väl modellen generaliserar på osedd data. Detta ger oss insikten hur bra modellen verkligen är på att generalisera på inte tidigare skådad data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec119bf8",
   "metadata": {},
   "source": [
    "5. Steg 1:<br>\n",
    "Först hämtas antal parametrar (features) från x_train och lagras i n_cols\n",
    "En sequenial modell instansieras i nn_model\n",
    "Ett första (dolt) lager med 100 neuroner skapas. Aktiveringsfunktion=\"relu\". Input_shape anger hur många features som finns vilka hämtas från n_cols.\n",
    "Vi adderar en dropout om 0.2 för att regularisera modellen.\n",
    "Ett andra (dolt) lager om 50 neuroner skapas. Aktiveringsfunktion=\"Relu\".\n",
    "Sist skapar vi ett output lager med 1 neuron för att få ut ett värde. Aktiveringsfunktion=\"Sigmoid\", vilket ger en sannolikhet (0 till 1).\n",
    "\n",
    "    Steg 2:<br>\n",
    "    Kompilering av modellen.\n",
    "    optimizer='adam' - används för att optimera modellen under träningen för att minimera förlusten.\n",
    "    loss binary_crossentropy = används vid klassificiering för att mäta true mot predicted.\n",
    "    metrics=['accuracy'] - används för att mätra hur bra vår modell presterar.\n",
    "\n",
    "    Steg 3:<br>\n",
    "    EarlyStopping(patience=5) - används för att avbryta träningen om modellen inte förbättras. Om den inte presterar bättre under 5 epoker i rad, stoppas träningen.\n",
    "    Modellen tränas genom fit. Vi tränar modellen genom indata x_train och y_train. 20% av träningsdatan används som valideringsdata för att mäta modellens prestanda under träning.\n",
    "    100 epoker anger hur många \"varv\" modellen potentiellt kan träna för att hitta bästa optimering.\n",
    "    epochs=100 - anger hur många epoker (antal försök) modellen har på sig att träna för att förbättra sitt resultat.\n",
    "    Callback=\"Early stopping\" anropar vår Early stopping med patience 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b82231",
   "metadata": {},
   "source": [
    "6. Att se till så att modellen inte blir överanpassad (Overfitted), dvs att den inte lär sig träningsdatan för väl. Då kommer den inte bli bra på att generalisera på ny data vilket leder till fler felaktiga prediktioner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428a5b0",
   "metadata": {},
   "source": [
    "7. Det är ett sätt för att motverka överanpassning. Detta sker genom att att vissa noder \"droppas\" vilket medför att modellen inte kan bli beroende av vissa noder vilket tvingar den till att göra alla noder mer \"självständiga\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22876d25",
   "metadata": {},
   "source": [
    "8. Earlystopping används för att sluta träna modellen när den inte längre presterar bättre. Vi anger ett värde för \"patience\" och när modellen presterat sämre x antal ggr i rad så stannar modellen. Den minns det bästa parametrara från innan den blev sämre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7bcd8",
   "metadata": {},
   "source": [
    "9. CNN, Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d014a8",
   "metadata": {},
   "source": [
    "10. Först identifierar den \"low-level features\" så som enklare former och färger.\n",
    "Därefter kombineras dessa för att skapa \"high-level features\" exempelvis ögon, mun m.m\n",
    "-För att en modell skall klassas som CNN krävs att minst ett av de dolda lagren är ett Convolutinal layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f19883",
   "metadata": {},
   "source": [
    "11. Den första raden sparar den tränade modellen i en fil med namnet \"model_file.keras\".\n",
    "Den andra raden laddar in den sparade modellen från filen och lagrar den i variabeln my_model, så att modellen kan användas igen utan att behöva tränas om."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352322f",
   "metadata": {},
   "source": [
    "12. CPU: Datorns processor. Styr och utför allmänna beräkningar.\n",
    "    <br>GPU: Grafikkortets processor. Gör många beräkningar samtidigt. Lämpar sig därför väl för bilder och ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb24d61",
   "metadata": {},
   "source": [
    "<h3>Självutvärdering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0dc716",
   "metadata": {},
   "source": [
    "1. Vad har varit roligast i kunskapskontrollen?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1c73e",
   "metadata": {},
   "source": [
    "2. Vilket betyg anser du att du ska ha och varför?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c2667",
   "metadata": {},
   "source": [
    "3. Vad har varit mest utmanande i arbetet och hur har du hanterat det?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
