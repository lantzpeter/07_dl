{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bdb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers #Installed package from https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in /home/peter/.local/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /home/peter/.local/lib/python3.10/site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: packaging in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.71.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.9.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow<2.20,>=2.19->tf-keras) (59.6.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/peter/.local/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: namex in /home/peter/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: rich in /home/peter/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: optree in /home/peter/.local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/peter/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/peter/.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/peter/.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/peter/.local/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/peter/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/peter/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/peter/.local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/peter/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4917c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-24 10:49:02.387835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748076542.402595    3791 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748076542.407009    3791 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748076542.419661    3791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076542.419697    3791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076542.419701    3791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748076542.419702    3791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-24 10:49:02.424076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random #I want the model to pick a random quote.\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from sentence_transformers import SentenceTransformer #Importing from Hugging face so i can use a local model for embedding.\n",
    "from pypdf import PdfReader\n",
    "import nltk #Importing so i can get whole quotes (rows) even if there is . in the sentence.\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"punkt_tab\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity #Importing for semantic searching.\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"API_KEY\"))\n",
    "model=SentenceTransformer('all-MiniLM-L6-v2') #Model used for local embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbcb47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a large language model, trained by Google.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content( #Running to see if the model is working, and can leaave me a response to my question.\n",
    "model=\"gemini-2.0-flash\",\n",
    "contents=\"Hi, who are you?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8844764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 173 0 (offset 0)\n",
      "Ignoring wrong pointing object 376 0 (offset 0)\n",
      "Ignoring wrong pointing object 393 0 (offset 0)\n",
      "Ignoring wrong pointing object 425 0 (offset 0)\n",
      "Ignoring wrong pointing object 427 0 (offset 0)\n",
      "Ignoring wrong pointing object 434 0 (offset 0)\n",
      "Ignoring wrong pointing object 652 0 (offset 0)\n",
      "Ignoring wrong pointing object 678 0 (offset 0)\n",
      "Ignoring wrong pointing object 781 0 (offset 0)\n",
      "Ignoring wrong pointing object 837 0 (offset 0)\n",
      "Ignoring wrong pointing object 840 0 (offset 0)\n",
      "Ignoring wrong pointing object 843 0 (offset 0)\n",
      "Ignoring wrong pointing object 854 0 (offset 0)\n",
      "Ignoring wrong pointing object 885 0 (offset 0)\n",
      "Ignoring wrong pointing object 929 0 (offset 0)\n",
      "Ignoring wrong pointing object 1050 0 (offset 0)\n",
      "Ignoring wrong pointing object 1092 0 (offset 0)\n",
      "Ignoring wrong pointing object 1125 0 (offset 0)\n",
      "Ignoring wrong pointing object 1138 0 (offset 0)\n",
      "Ignoring wrong pointing object 1140 0 (offset 0)\n",
      "Ignoring wrong pointing object 1149 0 (offset 0)\n",
      "Ignoring wrong pointing object 1157 0 (offset 0)\n",
      "Ignoring wrong pointing object 1161 0 (offset 0)\n",
      "Ignoring wrong pointing object 1163 0 (offset 0)\n",
      "Ignoring wrong pointing object 1173 0 (offset 0)\n",
      "Ignoring wrong pointing object 1200 0 (offset 0)\n",
      "Ignoring wrong pointing object 1202 0 (offset 0)\n",
      "Ignoring wrong pointing object 1276 0 (offset 0)\n",
      "Ignoring wrong pointing object 1280 0 (offset 0)\n",
      "Ignoring wrong pointing object 1290 0 (offset 0)\n",
      "Ignoring wrong pointing object 1425 0 (offset 0)\n",
      "Ignoring wrong pointing object 1452 0 (offset 0)\n",
      "Ignoring wrong pointing object 1512 0 (offset 0)\n",
      "Ignoring wrong pointing object 1551 0 (offset 0)\n",
      "Ignoring wrong pointing object 1578 0 (offset 0)\n",
      "Ignoring wrong pointing object 1588 0 (offset 0)\n",
      "Ignoring wrong pointing object 1645 0 (offset 0)\n",
      "Ignoring wrong pointing object 1650 0 (offset 0)\n",
      "Ignoring wrong pointing object 1674 0 (offset 0)\n",
      "Ignoring wrong pointing object 1677 0 (offset 0)\n",
      "Ignoring wrong pointing object 1725 0 (offset 0)\n",
      "Ignoring wrong pointing object 1735 0 (offset 0)\n",
      "Ignoring wrong pointing object 1753 0 (offset 0)\n",
      "Ignoring wrong pointing object 1755 0 (offset 0)\n",
      "Ignoring wrong pointing object 1793 0 (offset 0)\n",
      "Ignoring wrong pointing object 1795 0 (offset 0)\n",
      "Ignoring wrong pointing object 1952 0 (offset 0)\n",
      "Ignoring wrong pointing object 1969 0 (offset 0)\n",
      "Ignoring wrong pointing object 1978 0 (offset 0)\n",
      "Ignoring wrong pointing object 1990 0 (offset 0)\n",
      "Ignoring wrong pointing object 2059 0 (offset 0)\n",
      "Ignoring wrong pointing object 2068 0 (offset 0)\n",
      "Ignoring wrong pointing object 2076 0 (offset 0)\n",
      "Ignoring wrong pointing object 2079 0 (offset 0)\n",
      "Ignoring wrong pointing object 2081 0 (offset 0)\n",
      "Ignoring wrong pointing object 2083 0 (offset 0)\n",
      "Ignoring wrong pointing object 2085 0 (offset 0)\n",
      "Ignoring wrong pointing object 2087 0 (offset 0)\n",
      "Ignoring wrong pointing object 2089 0 (offset 0)\n",
      "Ignoring wrong pointing object 2091 0 (offset 0)\n",
      "Ignoring wrong pointing object 2093 0 (offset 0)\n",
      "Ignoring wrong pointing object 2095 0 (offset 0)\n",
      "Ignoring wrong pointing object 2097 0 (offset 0)\n",
      "Ignoring wrong pointing object 2099 0 (offset 0)\n",
      "Ignoring wrong pointing object 2101 0 (offset 0)\n",
      "Ignoring wrong pointing object 2103 0 (offset 0)\n",
      "Ignoring wrong pointing object 2105 0 (offset 0)\n",
      "Ignoring wrong pointing object 2107 0 (offset 0)\n",
      "Ignoring wrong pointing object 2280 0 (offset 0)\n",
      "Ignoring wrong pointing object 2298 0 (offset 0)\n",
      "Ignoring wrong pointing object 2300 0 (offset 0)\n",
      "Ignoring wrong pointing object 2302 0 (offset 0)\n",
      "Ignoring wrong pointing object 2306 0 (offset 0)\n",
      "Ignoring wrong pointing object 2310 0 (offset 0)\n",
      "Ignoring wrong pointing object 2329 0 (offset 0)\n",
      "Ignoring wrong pointing object 2331 0 (offset 0)\n",
      "Ignoring wrong pointing object 2333 0 (offset 0)\n",
      "Ignoring wrong pointing object 2337 0 (offset 0)\n",
      "Ignoring wrong pointing object 2369 0 (offset 0)\n",
      "Ignoring wrong pointing object 2371 0 (offset 0)\n",
      "Ignoring wrong pointing object 2373 0 (offset 0)\n",
      "Ignoring wrong pointing object 2377 0 (offset 0)\n",
      "Ignoring wrong pointing object 2427 0 (offset 0)\n",
      "Ignoring wrong pointing object 2436 0 (offset 0)\n",
      "Ignoring wrong pointing object 2443 0 (offset 0)\n",
      "Ignoring wrong pointing object 2456 0 (offset 0)\n",
      "Ignoring wrong pointing object 2459 0 (offset 0)\n",
      "Ignoring wrong pointing object 2472 0 (offset 0)\n",
      "Ignoring wrong pointing object 2474 0 (offset 0)\n",
      "Ignoring wrong pointing object 2488 0 (offset 0)\n",
      "Ignoring wrong pointing object 2490 0 (offset 0)\n",
      "Ignoring wrong pointing object 2493 0 (offset 0)\n",
      "Ignoring wrong pointing object 2545 0 (offset 0)\n",
      "Ignoring wrong pointing object 2559 0 (offset 0)\n",
      "Ignoring wrong pointing object 2569 0 (offset 0)\n",
      "Ignoring wrong pointing object 2594 0 (offset 0)\n",
      "Ignoring wrong pointing object 2629 0 (offset 0)\n",
      "Ignoring wrong pointing object 2773 0 (offset 0)\n",
      "Ignoring wrong pointing object 2886 0 (offset 0)\n",
      "Ignoring wrong pointing object 2921 0 (offset 0)\n",
      "Ignoring wrong pointing object 2949 0 (offset 0)\n",
      "Ignoring wrong pointing object 2968 0 (offset 0)\n",
      "Ignoring wrong pointing object 2974 0 (offset 0)\n",
      "Ignoring wrong pointing object 2996 0 (offset 0)\n",
      "Ignoring wrong pointing object 3012 0 (offset 0)\n",
      "Ignoring wrong pointing object 3128 0 (offset 0)\n",
      "Ignoring wrong pointing object 3159 0 (offset 0)\n",
      "Ignoring wrong pointing object 3171 0 (offset 0)\n",
      "Ignoring wrong pointing object 3189 0 (offset 0)\n",
      "Ignoring wrong pointing object 3254 0 (offset 0)\n",
      "Ignoring wrong pointing object 3257 0 (offset 0)\n",
      "Ignoring wrong pointing object 3260 0 (offset 0)\n",
      "Ignoring wrong pointing object 3263 0 (offset 0)\n",
      "Ignoring wrong pointing object 3329 0 (offset 0)\n",
      "Ignoring wrong pointing object 3331 0 (offset 0)\n",
      "Ignoring wrong pointing object 3336 0 (offset 0)\n",
      "Ignoring wrong pointing object 3338 0 (offset 0)\n",
      "Ignoring wrong pointing object 3399 0 (offset 0)\n",
      "Ignoring wrong pointing object 3411 0 (offset 0)\n",
      "Ignoring wrong pointing object 3434 0 (offset 0)\n",
      "Ignoring wrong pointing object 3464 0 (offset 0)\n",
      "Ignoring wrong pointing object 3474 0 (offset 0)\n",
      "Ignoring wrong pointing object 3485 0 (offset 0)\n",
      "Ignoring wrong pointing object 3516 0 (offset 0)\n",
      "Ignoring wrong pointing object 3528 0 (offset 0)\n",
      "Ignoring wrong pointing object 3577 0 (offset 0)\n",
      "Ignoring wrong pointing object 3593 0 (offset 0)\n",
      "Ignoring wrong pointing object 3603 0 (offset 0)\n",
      "Ignoring wrong pointing object 3624 0 (offset 0)\n",
      "Ignoring wrong pointing object 3639 0 (offset 0)\n",
      "Ignoring wrong pointing object 3644 0 (offset 0)\n",
      "Ignoring wrong pointing object 3735 0 (offset 0)\n",
      "Ignoring wrong pointing object 3885 0 (offset 0)\n",
      "Ignoring wrong pointing object 3889 0 (offset 0)\n",
      "Ignoring wrong pointing object 3911 0 (offset 0)\n",
      "Ignoring wrong pointing object 3934 0 (offset 0)\n",
      "Ignoring wrong pointing object 3969 0 (offset 0)\n",
      "Ignoring wrong pointing object 3979 0 (offset 0)\n",
      "Ignoring wrong pointing object 4033 0 (offset 0)\n",
      "Ignoring wrong pointing object 4036 0 (offset 0)\n",
      "Ignoring wrong pointing object 4038 0 (offset 0)\n",
      "Ignoring wrong pointing object 4102 0 (offset 0)\n",
      "Ignoring wrong pointing object 4311 0 (offset 0)\n",
      "Ignoring wrong pointing object 4313 0 (offset 0)\n",
      "Ignoring wrong pointing object 4315 0 (offset 0)\n",
      "Ignoring wrong pointing object 4356 0 (offset 0)\n",
      "Ignoring wrong pointing object 4434 0 (offset 0)\n",
      "Ignoring wrong pointing object 4436 0 (offset 0)\n",
      "Ignoring wrong pointing object 4439 0 (offset 0)\n",
      "Ignoring wrong pointing object 4443 0 (offset 0)\n",
      "Ignoring wrong pointing object 4446 0 (offset 0)\n",
      "Ignoring wrong pointing object 4449 0 (offset 0)\n",
      "Ignoring wrong pointing object 4452 0 (offset 0)\n",
      "Ignoring wrong pointing object 4455 0 (offset 0)\n",
      "Ignoring wrong pointing object 4612 0 (offset 0)\n",
      "Ignoring wrong pointing object 4631 0 (offset 0)\n",
      "Ignoring wrong pointing object 4674 0 (offset 0)\n",
      "Ignoring wrong pointing object 4720 0 (offset 0)\n",
      "Ignoring wrong pointing object 4723 0 (offset 0)\n",
      "Ignoring wrong pointing object 4731 0 (offset 0)\n",
      "Ignoring wrong pointing object 4734 0 (offset 0)\n",
      "Ignoring wrong pointing object 4803 0 (offset 0)\n",
      "Ignoring wrong pointing object 4826 0 (offset 0)\n",
      "Ignoring wrong pointing object 4852 0 (offset 0)\n",
      "Ignoring wrong pointing object 4946 0 (offset 0)\n",
      "Ignoring wrong pointing object 4969 0 (offset 0)\n",
      "Ignoring wrong pointing object 5060 0 (offset 0)\n",
      "Ignoring wrong pointing object 5106 0 (offset 0)\n",
      "Ignoring wrong pointing object 5108 0 (offset 0)\n",
      "Ignoring wrong pointing object 5144 0 (offset 0)\n",
      "Ignoring wrong pointing object 5168 0 (offset 0)\n",
      "Ignoring wrong pointing object 5177 0 (offset 0)\n",
      "Ignoring wrong pointing object 5180 0 (offset 0)\n",
      "Ignoring wrong pointing object 5213 0 (offset 0)\n",
      "Ignoring wrong pointing object 5237 0 (offset 0)\n",
      "Ignoring wrong pointing object 5252 0 (offset 0)\n",
      "Ignoring wrong pointing object 5270 0 (offset 0)\n",
      "Ignoring wrong pointing object 5334 0 (offset 0)\n",
      "Ignoring wrong pointing object 5338 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "reader_stat = PdfReader(\"pdf/islr.pdf\") #islr.pdf is the boook, An Introduction to Statistical Learning by Gareth James and more.\n",
    "text_stat = \"\"\n",
    "for page in reader_stat.pages:\n",
    "    text_stat += page.extract_text()\n",
    "\n",
    "reader_homl = PdfReader(\"pdf/homl.pdf\") #homl.pdf is the boook, Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow (ML/DL).\n",
    "text_homl = \"\"\n",
    "for page in reader_homl.pages:\n",
    "    text_homl += page.extract_text()\n",
    "\n",
    "reader_lotr = PdfReader(\"pdf/lotr.pdf\") #lotr.pdf is a pdf with some famous Lord of the ring quotes.\n",
    "text_lotr = \"\"\n",
    "for page in reader_lotr.pages:\n",
    "    text_lotr += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c0fa814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunking(text, max_tokens=100): #Creating funtion for Semantic chunking to use in next step.\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Check if adding the sentence keeps the chunk within the token limit\n",
    "        if len((current_chunk + \" \" + sentence).split()) <= max_tokens:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a973f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_stat = semantic_chunking(text_stat, max_tokens=100)\n",
    "chunks_homl = semantic_chunking(text_homl, max_tokens=100)\n",
    "\n",
    "sentences_lotr = sent_tokenize(text_lotr) #Not using Semantic search because i only have short quotes in this data.\n",
    "sentences_lotr = [s.strip() for s in sentences_lotr if len(s.strip()) > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff98cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAT chunks: 2530\n",
      "HOML chunks: 3130\n",
      "TOTAL chunks: 5660\n"
     ]
    }
   ],
   "source": [
    "print(\"STAT chunks:\", len(chunks_stat))\n",
    "print(\"HOML chunks:\", len(chunks_homl))\n",
    "print(\"TOTAL chunks:\", len(chunks_stat) + len(chunks_homl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2dd9e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skipping broken chunk in index 491 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 902 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 965 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 968 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 995 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 1007 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 1296 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 2148 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 2150 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      " Skipping broken chunk in index 2179 → TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n"
     ]
    }
   ],
   "source": [
    "valid_chunks_homl = [] #Had problem with creating embedding for homl, need to clean out bad/non string chunks.\n",
    "\n",
    "for i, chunk in enumerate(chunks_homl):\n",
    "    try:\n",
    "        model.encode([chunk])  # testing one each turn\n",
    "        valid_chunks_homl.append(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\" Skipping broken chunk in index {i} → {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cc3ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving text for later use\n",
    "os.makedirs(\"texts\", exist_ok=True)\n",
    "\n",
    "# STAT\n",
    "with open(\"texts/texts_stat.json\", \"w\") as f:\n",
    "    json.dump(chunks_stat, f)\n",
    "\n",
    "# HOML\n",
    "with open(\"texts/texts_homl.json\", \"w\") as f:\n",
    "    json.dump(valid_chunks_homl, f)\n",
    "\n",
    "# LOTR – varje citat är en mening\n",
    "with open(\"texts/texts_lotr.json\", \"w\") as f:\n",
    "    json.dump(sentences_lotr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f5707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 98/98 [00:03<00:00, 30.86it/s]\n",
      "Batches: 100%|██████████| 80/80 [00:02<00:00, 28.24it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 101.73it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_homl = model.encode(valid_chunks_homl, show_progress_bar=True) #Using the filtered/working chunks for homl.\n",
    "embeddings_stat = model.encode(chunks_stat, show_progress_bar=True)\n",
    "embeddings_lotr = model.encode(sentences_lotr, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfb3b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating save function for easier saving.\n",
    "def save(texts, embeddings, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump([{\"text\": t, \"embedding\": e.tolist()} for t, e in zip(texts, embeddings)], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fbf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Savinig embeddings for later use.\n",
    "save(chunks_stat, embeddings_stat, \"embeddings/embeddings_stat.json\")\n",
    "save(valid_chunks_homl, embeddings_homl, \"embeddings/embeddings_homl.json\")\n",
    "save(sentences_lotr, embeddings_lotr, \"embeddings/embeddings_lotr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f704399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading embeddings to save time.\n",
    "with open(\"embeddings/embeddings_stat.json\") as f:\n",
    "    data_stat = json.load(f)\n",
    "texts_stat = [d[\"text\"] for d in data_stat]\n",
    "embeddings_stat = np.array([d[\"embedding\"] for d in data_stat])\n",
    "\n",
    "with open(\"embeddings/embeddings_homl.json\") as f:\n",
    "    data_homl = json.load(f)\n",
    "texts_homl = [d[\"text\"] for d in data_homl]\n",
    "embeddings_homl = np.array([d[\"embedding\"] for d in data_homl])\n",
    "\n",
    "with open(\"embeddings/embeddings_lotr.json\") as f:\n",
    "    data_lotr = json.load(f)\n",
    "texts_lotr = [d[\"text\"] for d in data_lotr]\n",
    "embeddings_lotr = np.array([d[\"embedding\"] for d in data_lotr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d3ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tagging text so i know where the output came frome\n",
    "texts_stat_tagged = [f\"[STAT] {text}\" for text in texts_stat]\n",
    "texts_homl_tagged = [f\"[HOML] {text}\" for text in texts_homl]\n",
    "\n",
    "#Combining both stat and homl so the model can search in both.\n",
    "texts_combined = texts_stat_tagged + texts_homl_tagged\n",
    "embeddings_combined = np.vstack([embeddings_stat, embeddings_homl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57855a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, texts, embeddings, model, k=5):\n",
    "    query_emb = model.encode([query])\n",
    "    similarity_scores = cosine_similarity(query_emb, embeddings)[0]\n",
    "    top_indices = similarity_scores.argsort()[-k:][::-1]\n",
    "    return [(texts[i], similarity_scores[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8551eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful assistant who explains statistical and machine learning concepts clearly.\n",
    "\n",
    "You will receive a question along with some context. You must base your answer **only on the provided context**, and not on any external knowledge.\n",
    "\n",
    "If the context does not contain enough information to answer the question, simply reply:  \n",
    "**\"I don't know.\"** Do not attempt to guess or invent details.\n",
    "\n",
    "Please write your answer in clear, simple language, and structure it into well-formed paragraphs.\n",
    "\n",
    "Use a slightly whimsical tone, as if you were Gandalf explaining it over a campfire.\n",
    "\n",
    "---\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "To make it more entertaining, include a Lord of the Rings quote that fits the context. You may **tweak the quote slightly** to make it funnier or more related to the topic.\n",
    "\n",
    "At the end of your answer, include the quote on its own line, followed by the name of the character who said it.\n",
    "For example:\n",
    "\"There is always hope.\" – Aragorn\n",
    "\n",
    "Quote:\n",
    "{lotr_quote}\" – {lotr_author}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291e7ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_prompt(query, texts, embeddings, model, k=5):\n",
    "    context_chunks = semantic_search(query, texts, embeddings, model, k)\n",
    "    context = \"\\n\\n\".join([chunk for chunk, score in context_chunks])\n",
    "    return f\"Question: {query}\\n\\nContext:\\n{context}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d655a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lotr_author(quote): #I want to present the author of the quote later on so thats why I extract it seperatly.\n",
    "    if \"–\" in quote:\n",
    "        return quote.split(\"–\")[-1].strip()\n",
    "    elif \"-\" in quote:\n",
    "        return quote.split(\"-\")[-1].strip()\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5135b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(system_prompt, query, texts, embeddings, model, client, lotr_quotes, gemini_model=\"gemini-2.0-flash\"):\n",
    "    lotr_quote = random.choice(lotr_quotes)\n",
    "    user_prompt = generate_user_prompt(query, texts, embeddings, model)\n",
    "\n",
    "    filled_prompt = system_prompt.format(\n",
    "        context=user_prompt.split(\"Context:\\n\")[1],\n",
    "        query=query,\n",
    "        lotr_quote=lotr_quote,\n",
    "        lotr_author = extract_lotr_author(lotr_quote)\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "    model=gemini_model,\n",
    "    contents=filled_prompt\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "636bdf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halt, traveler! Gather 'round the fire, and let's speak of the Normal Distribution, a shape as familiar to statisticians as the Shire is to Hobbits.\n",
      "\n",
      "Imagine a bell, a grand, symmetrical bell. That's the shape of a Normal Distribution, also known as a Gaussian distribution. It describes how the values of a feature, like the heights of Elves or the prices of Lembas bread, are spread out. Most values cluster around the average, the mean – that's the peak of our bell. Then, as you move away from the mean, the values become less common, tapering off on either side.\n",
      "\n",
      "There's even a rule of thumb to remember, a bit like an Elven rhyme: \"68-95-99.7\". About 68% of the values fall within one standard deviation (σ) of the mean, 95% within two σ, and a whopping 99.7% within three σ. This \"standard deviation\" measures how spread out the values are. A small standard deviation means the bell is narrow and tall, most values are close to the mean. A large standard deviation means the bell is wide and flat, the values are more scattered.\n",
      "\n",
      "Indeed, the Normal Distribution is everywhere, from the random vectors sampled by K.random_normal() with a mean μ and standard deviation σ, to the distribution of test statistics under the null hypothesis in statistical tests. Even the humble t-distribution, when its \"n\" value surpasses around 30, starts resembling the shape of our beloved bell.\n",
      "\n",
      "\"This distribution was appointed to you, and if you do not find a way, no one will.\" – Gandalf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(\n",
    "    system_prompt,\n",
    "    query=\"What is Normal Distribution?\",\n",
    "    texts=texts_combined,\n",
    "    embeddings=embeddings_combined,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    lotr_quotes=texts_lotr\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a24e9",
   "metadata": {},
   "source": [
    "<h3>Evaluation of the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be2ee3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = [ #Creating questions with ideal answers so i can evalute the models performance.\n",
    "{\"question\": \"What is a common performance metric for regression problems?\",\n",
    " \"ideal_answer\": \"A typical performance measure for regression problems is the Root Mean Square Error (RMSE).\"\n",
    "},\n",
    "{\"question\": \"When we fit a linear regression model to a particular data set, many problems may occur. which is the most common?:\",\n",
    " \"ideal_answer\": \"Common problems in linear regression include non-linearity, heteroscedasticity, correlated errors, outliers, high-leverage points, and collinearity.\"\n",
    "},\n",
    "{\"question\": \"How many horses are in Lord of the Rings?\",\n",
    " \"ideal_answer\": \"There is no relevant information about horses in the provided context.\"\n",
    "},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbbb7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_system_prompt = \"\"\"You are an intelligent evaluation system that grades the quality of an AI assistant's answer.\n",
    "\n",
    "You will be given:\n",
    "- A user question\n",
    "- An ideal answer (the key facts that must be included)\n",
    "- The assistant's generated answer (which may have a whimsical tone)\n",
    "\n",
    "Your task is to evaluate the assistant's answer based on factual correctness and coverage of key ideas, **regardless of style**.\n",
    "\n",
    "Score:\n",
    "- 1.0 if the assistant includes the key information from the ideal answer, even if expressed creatively or with humor\n",
    "- 0.5 if the answer is partially correct or misses important parts\n",
    "- 0.0 if the answer is incorrect or irrelevant\n",
    "\n",
    "Also include a brief motivation for your score.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7ad8f",
   "metadata": {},
   "source": [
    "Testing the model on a questions that does exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "740937f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n",
      "\n",
      "Motivation:The AI assistant correctly identifies Root Mean Square Error (RMSE) as a common performance metric for regression problems. It also mentions Mean Squared Error (MSE) which is closely related and also correct. The answer is creative, but contains the correct information.\n"
     ]
    }
   ],
   "source": [
    "query = validation_data[0][\"question\"]\n",
    "response = generate_response(\n",
    "    system_prompt=system_prompt,\n",
    "    query=query,\n",
    "    texts=texts_combined,\n",
    "    embeddings=embeddings_combined,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    lotr_quotes=texts_lotr\n",
    ")\n",
    "\n",
    "evaluation_prompt = f\"\"\"Question: {query}\n",
    "AI-assistent answer: {response}\n",
    "Ideal Answer: {validation_data[0]['ideal_answer']}\"\"\"\n",
    "\n",
    "evaluation_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=evaluation_prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=evaluation_system_prompt,\n",
    "        temperature=0.0\n",
    "    )\n",
    ")\n",
    "\n",
    "print(evaluation_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34eb79",
   "metadata": {},
   "source": [
    "Testing the model on a questions that does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab47dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many horses are in Lord of the Rings?',\n",
       " 'ideal_answer': 'There is no relevant information about horses in the provided context.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14f8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0\n",
      "\n",
      "Motivation:The AI assistant correctly states that it does not know the answer to the question. It also acknowledges the lack of relevant information. The whimsical tone does not detract from the accuracy of the response.\n"
     ]
    }
   ],
   "source": [
    "query = validation_data[2][\"question\"]\n",
    "response = generate_response(\n",
    "    system_prompt=system_prompt,\n",
    "    query=query,\n",
    "    texts=texts_combined,\n",
    "    embeddings=embeddings_combined,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    lotr_quotes=texts_lotr\n",
    ")\n",
    "\n",
    "evaluation_prompt = f\"\"\"Question: {query}\n",
    "AI-assistent answer: {response}\n",
    "Ideal Answer: {validation_data[2]['ideal_answer']}\"\"\"\n",
    "\n",
    "evaluation_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=evaluation_prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=evaluation_system_prompt,\n",
    "        temperature=0.0\n",
    "    )\n",
    ")\n",
    "\n",
    "print(evaluation_response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
